# @package _global_
defaults:
- /model: roberta
- /dataset: fhm_finegrained
- /datamodule: roberta_datamodul
- /trainer: single_gpu_trainer
- /metric:
  - accuracy
  - auroc
- /hydra: experiment
- _self_
    
# Mandatory Configuration Parameters
model:
  cls_dict:
    hate: 2
  optimizers: 
  - class_path: torch.optim.Adam
    lr: 2e-5

dataset:
  dataset_class: datasets.fhm_finegrained.TextClassificationDataset
  text_template: "{interpretation}"
  labels:
  - hate
  auxiliary_dicts:
    train:
      interpretation: /mnt/sda/mshee/datasets/memes/fhm_finegrained/interpretations/mPLUG.json
      caption: /mnt/sda/mshee/datasets/memes/fhm_finegrained/captions/mPLUG.json
    validate:
      interpretation: /mnt/sda/mshee/datasets/memes/fhm_finegrained/interpretations/mPLUG.json
      caption: /mnt/sda/mshee/datasets/memes/fhm_finegrained/captions/mPLUG.json
    test:
      interpretation: /mnt/sda/mshee/datasets/memes/fhm_finegrained/interpretations/mPLUG.json
      caption: /mnt/sda/mshee/datasets/memes/fhm_finegrained/captions/mPLUG.json
    predict:
      interpretation: /mnt/sda/mshee/datasets/memes/fhm_finegrained/interpretations/mPLUG.json
      caption: /mnt/sda/mshee/datasets/memes/fhm_finegrained/captions/mPLUG.json

datamodule:
  tokenizer_class_or_path: roberta-large

monitor_metric: validate_hate_average
monitor_mode: max
save_top_ks: 1

# Experiment settings
experiment_name: baseline/fhm_finegrained_mPLUG_RoBERTa

# Job settings
hydra.verbose: True
seed_everything: 1111
overwrite: False
action: ???
# @package _global_
defaults:
- /model: roberta
- /dataset: mami
- /datamodule: roberta_datamodul
- /trainer: single_gpu_trainer
- /metric:
  - accuracy
  - auroc
- /hydra: experiment
- _self_
    
# Mandatory Configuration Parameters
model:
  cls_dict:
    misogynous: 2
  optimizers: 
  - class_path: torch.optim.Adam
    lr: 2e-5

dataset:
  dataset_class: datasets.mami.TextClassificationDataset
  text_template: "{interpretation}"
  labels:
  - misogynous
  auxiliary_dicts:
    train:
      interpretation: /mnt/data1/datasets/memes/mami/interpretations/train/InstructBLIP.json
      # caption: /mnt/data1/datasets/memes/mami/captions/InstructBLIP.json
    validate:
      interpretation: /mnt/data1/datasets/memes/mami/interpretations/test/InstructBLIP.json
      # caption: /mnt/data1/datasets/memes/mami/captions/InstructBLIP.json
    test:
      interpretation: /mnt/data1/datasets/memes/mami/interpretations/test/InstructBLIP.json
      # caption: /mnt/data1/datasets/memes/mami/captions/InstructBLIP.json
    predict:
      interpretation: /mnt/data1/datasets/memes/mami/interpretations/test/InstructBLIP.json
      # caption: /mnt/data1/datasets/memes/mami/captions/InstructBLIP.json

datamodule:
  tokenizer_class_or_path: roberta-large
  batch_size: 16

trainer:
  accumulate_grad_batches: 2

monitor_metric: validate_misogynous_average
monitor_mode: max
save_top_ks: 1

# Experiment settings
experiment_name: baseline/mami_InstructBLIP_RoBERTa

# Job settings
hydra.verbose: True
seed_everything: 1111
overwrite: False
action: ???
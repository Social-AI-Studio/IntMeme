# @package _global_
defaults:
- /model: frozen_mime
- /dataset: fhm_finegrained
- /datamodule: mime_datamodule
- /trainer: single_gpu_trainer
- /metric:
  - accuracy
  - auroc
- /hydra: experiment
- _self_
    
# Mandatory Configuration Parameters
model:
  cls_dict: 
    hate: 2
  intermediate_cls_hidden_dims: []
  freeze_pretrain: False
  optimizers: 
  - class_path: torch.optim.Adam
    lr: 2e-5

dataset:
  dataset_class: datasets.fhm_finegrained.MimeDataset
  text_template: "{interpretation}"
  labels:
  - hate
  auxiliary_dicts:
    train:
      interpretation: /mnt/sda/mshee/datasets/memes/fhm_finegrained/interpretations/InstructBLIP.json
      caption: /mnt/sda/mshee/datasets/memes/fhm_finegrained/captions/InstructBLIP.json
    validate:
      interpretation: /mnt/sda/mshee/datasets/memes/fhm_finegrained/interpretations/InstructBLIP.json
      caption: /mnt/sda/mshee/datasets/memes/fhm_finegrained/captions/InstructBLIP.json
    test:
      interpretation: /mnt/sda/mshee/datasets/memes/fhm_finegrained/interpretations/InstructBLIP.json
      caption: /mnt/sda/mshee/datasets/memes/fhm_finegrained/captions/InstructBLIP.json
    predict:
      interpretation: /mnt/sda/mshee/datasets/memes/fhm_finegrained/interpretations/InstructBLIP.json
      caption: /mnt/sda/mshee/datasets/memes/fhm_finegrained/captions/InstructBLIP.json
    

datamodule:
  multimodal_tokenizer_class_or_path: facebook/flava-full
  rc_tokenizer_class_or_path: roberta-large
  frcnn_class_or_path: None
  batch_size: 16

trainer:
  accumulate_grad_batches: 2

monitor_metric: validate_hate_average
monitor_mode: max
save_top_ks: 1

# Experiment settings
experiment_name: fhm_fg_mlps/mime_InstructBLIP_interpretation

# Job settings
hydra.verbose: True
seed_everything: 1111
overwrite: False
action: ???